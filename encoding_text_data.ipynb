{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNu6eED66ZYIXMB7Ivc1Pls"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import string\n",
        "import re\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "tn6VNMF2T-dI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization Steps\n",
        "In this exercise, we will code our own tokenizer from scratching using base Python. This exercise will help us get to know see some of the tokenization steps better."
      ],
      "metadata": {
        "id": "XLA5QyNXLRqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Sample Text\n",
        "Let's first define some sample text we will use to test our tokenization steps."
      ],
      "metadata": {
        "id": "7mHhWGgWT1jQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = '''Mr. Louis continued to say, \"Penguins are important,\n",
        "but we mustn't forget the nuumber 1 priority: the READER!\"\n",
        "'''\n",
        "\n",
        "print(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yq69XZa4LR--",
        "outputId": "120f4153-231a-4352-a3e1-6fcd14cefc12"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mr. Louis continued to say, \"Penguins are important,\n",
            "but we must not forget the number 1 priority: the READER!\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization\n",
        "- convert text into lowercase\n",
        "- remove accented characters"
      ],
      "metadata": {
        "id": "ZOOgOdduUVHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text: str) -> str:\n",
        "    acceptable_characters = (\n",
        "        string.ascii_letters\n",
        "        + string.digits\n",
        "        + string.punctuation\n",
        "        + string.whitespace\n",
        "    )\n",
        "    normalized_text = ''.join(\n",
        "        filter(lambda letter: letter in  acceptable_characters, text)\n",
        "    )\n",
        "    # Make text lower-case\n",
        "    normalized_text = normalized_text.lower()\n",
        "    return normalized_text"
      ],
      "metadata": {
        "id": "rDHsVD0yUTqS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the normalization\n",
        "normalize_text(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "XvWAsshGVgC2",
        "outputId": "41bd791c-9c40-4a26-9309-83d4460aaaad"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mr. louis continued to say, \"penguins are important,\\nbut we must not forget the number 1 priority: the reader!\"\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretokenization\n",
        "This step will take in the normalized text and pretokenize the text into a list of smaller pieces."
      ],
      "metadata": {
        "id": "ciWfy-LRW5Hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretokenize_text(text: str) -> list[str]:\n",
        "    smaller_pieces = text.split()\n",
        "    return smaller_pieces"
      ],
      "metadata": {
        "id": "80BDwmG0VlZC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out our pretokenization step (after normalizing the text)\n",
        "normalized_text = normalize_text(sample_text)\n",
        "pretokenize_text(normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdMfJAgKXO0C",
        "outputId": "0ec08702-80d2-48f1-adc3-a3f0d21ecafa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mr.',\n",
              " 'louis',\n",
              " 'continued',\n",
              " 'to',\n",
              " 'say,',\n",
              " '\"penguins',\n",
              " 'are',\n",
              " 'important,',\n",
              " 'but',\n",
              " 'we',\n",
              " 'must',\n",
              " 'not',\n",
              " 'forget',\n",
              " 'the',\n",
              " 'number',\n",
              " '1',\n",
              " 'priority:',\n",
              " 'the',\n",
              " 'reader!\"']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "This step will take in the list of pretokenized pieces (after the text has been normalized) into the tokens that will be used."
      ],
      "metadata": {
        "id": "RmvJcabwXjlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine normalization and pretokenization steps before breaking things further\n",
        "def tokenize_text(text: str) -> list[str]:\n",
        "    normalized_text: str = normalize_text(text)\n",
        "    pertokenized_text: list[str] = pretokenize_text(normalized_text)\n",
        "    tokens = []\n",
        "    for word in pertokenized_text:\n",
        "        tokens.extend(\n",
        "            re.findall(\n",
        "                f'[\\w]+|[{string.punctuation}]', # Split word at punctuations\n",
        "                word,\n",
        "            )\n",
        "        )\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "NCjmtJwgXhz8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out our tokenization (that uses normalizing & pretokenizing functions)\n",
        "tokenize_text(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lInADkVaKEZ",
        "outputId": "0ab89691-3ac4-4e1d-cafe-cd5e621e4e60"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mr',\n",
              " '.',\n",
              " 'louis',\n",
              " 'continued',\n",
              " 'to',\n",
              " 'say',\n",
              " ',',\n",
              " '\"',\n",
              " 'penguins',\n",
              " 'are',\n",
              " 'important',\n",
              " ',',\n",
              " 'but',\n",
              " 'we',\n",
              " 'must',\n",
              " 'not',\n",
              " 'forget',\n",
              " 'the',\n",
              " 'number',\n",
              " '1',\n",
              " 'priority',\n",
              " ':',\n",
              " 'the',\n",
              " 'reader',\n",
              " '!',\n",
              " '\"']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Postprocessing\n",
        "This final step will take in the list of tokens from the original text and add any special tokens to the text."
      ],
      "metadata": {
        "id": "CTAsb_mpaUCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Useful for some tasks\n",
        "def postprocess_tokens(tokens: list[str]) -> list[str]:\n",
        "  # Add beginning and end of sequence tokens to our tokenized text\n",
        "    bos_token = '[BOS]'\n",
        "    eos_token = '[EOS]'\n",
        "    updated_tokens = (\n",
        "        [bos_token]\n",
        "        + tokens\n",
        "        + [eos_token]\n",
        "    )\n",
        "    return updated_tokens"
      ],
      "metadata": {
        "id": "xSUNNGJoaR8X"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test full pipeline (normalizing, pretokenizing, tokenizing & postprocessing)\n",
        "tokens = tokenize_text(sample_text)\n",
        "tokens = postprocess_tokens(tokens)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OagJys2a-kj",
        "outputId": "23fd2df4-4afb-42b8-c062-62a5188aa37e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'must', 'not', 'forget', 'the', 'number', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding & Decoding\n",
        "## Encoding Text to Token IDs\n",
        "Create an encoder (`encode()`) that will encode the token strings to integer IDs by defining how to map each token to a unique ID."
      ],
      "metadata": {
        "id": "Ov4_GUfrdNmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample corpus (normally this would be much bigger)\n",
        "sample_corpus = (\n",
        "    '''Mr. Louis continued to say, \"Penguins are important, \\nbut we mustn't forget the nuumber 1 priority: the READER!\"''',\n",
        "    '''BRUTUS:\\nHe's a lamb indeed, that baes like a bear.''',\n",
        "    '''Both by myself and many other friends:\\mBut he, his own affections' counsellor,\\nIs to himself--I will not say how true--\\nBut to himself so secret and so close,'''\n",
        ")"
      ],
      "metadata": {
        "id": "vnmp9sGxdMkV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an encoder to transform token strings to IDs using the sample\n",
        "# corpus as the basis of your encoding\n",
        "\n",
        "unique_tokens = set()\n",
        "for text in sample_corpus:\n",
        "    tokens_from_text = tokenize_text(text)\n",
        "    tokens_from_text = postprocess_tokens(tokens_from_text)\n",
        "    unique_tokens.update(tokens_from_text)\n",
        "\n",
        "# Create mapping (dictionary) for unique tokens using arbitraty & unique IDs\n",
        "token2id = defaultdict(lambda : 0) # Allow for unknown tokens to map to 0\n",
        "token2id |= {\n",
        "    token: idx\n",
        "    for idx, token in enumerate(unique_tokens, 1) # Skip 0 (represents unknown)\n",
        "}\n",
        "\n",
        "# A mapping for IDs to convert back to token\n",
        "id2token = defaultdict(lambda: '[UNK]') # Allow for unknown token ('[UNK]')\n",
        "id2token |= {\n",
        "    idx: token\n",
        "    for token, idx in token2id.items()\n",
        "}\n",
        "\n",
        "\n",
        "def encode(tokens: list[str]) -> list[int]:\n",
        "    encoded_tokens = [token2id[token] for token in tokens]\n",
        "    return encoded_tokens\n"
      ],
      "metadata": {
        "id": "paKHq_UppQ7o"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test `encode()`"
      ],
      "metadata": {
        "id": "C-e8qFoipR7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use sample text for testing\n",
        "sample_text = sample_corpus[0]\n",
        "# Create tokens (to be fed to encode())\n",
        "tokens = tokenize_text(sample_text)\n",
        "tokens = postprocess_tokens(tokens)\n",
        "print(f'Tokens:\\n{tokens}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHogIiWLpT2j",
        "outputId": "ffebd3cc-c5e8-4a3d-a99c-3ff3712d1b27"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test encode()\n",
        "encoded_tokens = encode(tokens)\n",
        "print(f'Encoded Tokens:\\n{encoded_tokens}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJGzGAnPpV29",
        "outputId": "65a495fa-6bf7-4576-b6c7-7cd1aa388c18"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Tokens:\n",
            "[32, 5, 38, 48, 20, 21, 3, 40, 36, 26, 25, 8, 40, 39, 24, 49, 1, 4, 6, 50, 41, 34, 37, 29, 50, 19, 12, 36, 13]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoding Token IDs to Text\n",
        "Based on the encoder we created (`encode()`), create a decoder(`decode()`) to take a list of token IDs and map them to their associated token."
      ],
      "metadata": {
        "id": "tHHdd5uipZqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(ids: list[int]) -> list[str]:\n",
        "    token_strings = [id2token[idx] for idx in ids]\n",
        "    return token_strings"
      ],
      "metadata": {
        "id": "Q6s31mp9pbjO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test `decode()`"
      ],
      "metadata": {
        "id": "Onsx4zFqpoxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use sample text for testing\n",
        "sample_text = sample_corpus[0]\n",
        "# Create tokens\n",
        "tokens = tokenize_text(sample_text)\n",
        "tokens = postprocess_tokens(tokens)\n",
        "print(f'Tokens:\\n{tokens}\\n')\n",
        "\n",
        "# Create token IDs (to be fed to decode())\n",
        "encoded_tokens = encode(tokens)\n",
        "print(f'Encoded Tokens:\\n{encoded_tokens}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TJmwtmepnyu",
        "outputId": "386b5ac2-dcfb-4bef-a606-b0a9e8a0df16"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n",
            "\n",
            "Encoded Tokens:\n",
            "[32, 5, 38, 48, 20, 21, 3, 40, 36, 26, 25, 8, 40, 39, 24, 49, 1, 4, 6, 50, 41, 34, 37, 29, 50, 19, 12, 36, 13]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out decode()\n",
        "decoded_tokens = decode(encoded_tokens)\n",
        "print(f'Decoded Tokens:\\n{decoded_tokens}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52xF4SOlpsEH",
        "outputId": "705a43f7-838f-4b2b-a70f-c3c8db408834"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded Tokens:\n",
            "['[BOS]', 'mr', '.', 'louis', 'continued', 'to', 'say', ',', '\"', 'penguins', 'are', 'important', ',', 'but', 'we', 'mustn', \"'\", 't', 'forget', 'the', 'nuumber', '1', 'priority', ':', 'the', 'reader', '!', '\"', '[EOS]']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}