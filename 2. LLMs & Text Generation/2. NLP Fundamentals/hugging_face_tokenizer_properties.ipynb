{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPglxB3NjXvaVxG3Te1kc89"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "n6WEKjPzF7Q8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Properties of Hugging Face's Tokenizers\n",
        "\n",
        "There are a lot of great features when using tokenizers in Hugging Face that can make it very simple to try out and use different modules. Here we'll briefly discuss some properties that can be useful.\n",
        "\n",
        "We'll load a couple of different models:\n",
        "* bert-base-cased\n",
        "* xlm-roberta-base\n",
        "* google/pegasus-xsum\n",
        "* allenai/longformer-base-4096"
      ],
      "metadata": {
        "id": "D83yj4TbF_zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = {\n",
        "    \"bert-base-cased\",\n",
        "    \"xlm-roberta-base\",\n",
        "    \"google/pegasus-xsum\",\n",
        "    \"allenai/longformer-base-4096\",\n",
        "}\n",
        "\n",
        "model_tokenizers = {\n",
        "    model_name: AutoTokenizer.from_pretrained(model_name)\n",
        "    for model_name in model_names\n",
        "}"
      ],
      "metadata": {
        "id": "t2tX6ugbGoCI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`model_max_length`\n",
        "\n",
        "Many models that tokenizers are associated with can only take in a maximum number of tokens and so the tokenizer might not be equipped to encode a very long sequence. It might not always be relevant, but you can find this length with `.model_max_length`"
      ],
      "metadata": {
        "id": "G7TPcM3jIS2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, temp_tokenizer in model_tokenizers.items():\n",
        "  max_length = temp_tokenizer.model_max_length\n",
        "  print(f\"{model_name}\\n\\tmax length: {max_length}\")\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgCmlQytInur",
        "outputId": "8cb98842-8914-4b92-f37d-c87c1f1708a1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert-base-cased\n",
            "\tmax length: 512\n",
            "\n",
            "\n",
            "allenai/longformer-base-4096\n",
            "\tmax length: 1000000000000000019884624838656\n",
            "\n",
            "\n",
            "xlm-roberta-base\n",
            "\tmax length: 512\n",
            "\n",
            "\n",
            "google/pegasus-xsum\n",
            "\tmax length: 512\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Special Tokens\n",
        "\n",
        "Different tokenizers will have different special tokens defined. They might have tokens representing:\n",
        "* Unknown token\n",
        "* Beginning of sequence token\n",
        "* Separator token\n",
        "* Token used for padding\n",
        "* Classifier token\n",
        "* Token used for masking values\n",
        "Additionally, there may be multiple subtypes of each special token. For example, some tokenizers have multiple different unknown tokens(e.g. `<unk>` and `<unk_2>`)"
      ],
      "metadata": {
        "id": "9_P1YBd1J4V1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, temp_tokenizer in model_tokenizers.items():\n",
        "  special_tokens = temp_tokenizer.all_special_tokens\n",
        "  print(f\"{model_name}\\n\\tspecial tokens: {special_tokens}\")\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guYTFyzSKS6p",
        "outputId": "4100dd94-e4f4-44c7-8953-42ca20bc3fd3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert-base-cased\n",
            "\tspecial tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
            "\n",
            "\n",
            "allenai/longformer-base-4096\n",
            "\tspecial tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
            "\n",
            "\n",
            "xlm-roberta-base\n",
            "\tspecial tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
            "\n",
            "\n",
            "google/pegasus-xsum\n",
            "\tspecial tokens: ['</s>', '<unk>', '<pad>', '<mask_2>', '<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also it's possible to call the specific token we're interested in to see its representation."
      ],
      "metadata": {
        "id": "c7451l1KNaSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_tokenizers['bert-base-cased'].unk_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7HiAA1Q9LKU3",
        "outputId": "259f515f-f05e-4861-a9c8-933ac616f385"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[UNK]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name, temp_tokenizer in model_tokenizers.items():\n",
        "  print(f\"{model_name}\")\n",
        "  print(f'\\tUnknown: \\n\\t\\t{temp_tokenizer.unk_token=}')\n",
        "  print(f'\\tBeginning of Sequence: \\n\\t\\t{temp_tokenizer.bos_token=}')\n",
        "  print(f'\\tEnd of Sequence: \\n\\t\\t{temp_tokenizer.eos_token=}')\n",
        "  print(f'\\tMask: \\n\\t\\t{temp_tokenizer.mask_token=}')\n",
        "  print(f'\\tSequence Separator: \\n\\t\\t{temp_tokenizer.sep_token=}')\n",
        "  print(f'\\tClass of Input: \\n\\t\\t{temp_tokenizer.cls_token=}')\n",
        "  print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWEHQlMIOXJ1",
        "outputId": "3182e8ac-26cf-42e6-ded3-cd899aca5d26"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert-base-cased\n",
            "\tUnknown: \n",
            "\t\ttemp_tokenizer.unk_token='[UNK]'\n",
            "\tBeginning of Sequence: \n",
            "\t\ttemp_tokenizer.bos_token=None\n",
            "\tEnd of Sequence: \n",
            "\t\ttemp_tokenizer.eos_token=None\n",
            "\tMask: \n",
            "\t\ttemp_tokenizer.mask_token='[MASK]'\n",
            "\tSequence Separator: \n",
            "\t\ttemp_tokenizer.sep_token='[SEP]'\n",
            "\tClass of Input: \n",
            "\t\ttemp_tokenizer.cls_token='[CLS]'\n",
            "\n",
            "\n",
            "allenai/longformer-base-4096\n",
            "\tUnknown: \n",
            "\t\ttemp_tokenizer.unk_token='<unk>'\n",
            "\tBeginning of Sequence: \n",
            "\t\ttemp_tokenizer.bos_token='<s>'\n",
            "\tEnd of Sequence: \n",
            "\t\ttemp_tokenizer.eos_token='</s>'\n",
            "\tMask: \n",
            "\t\ttemp_tokenizer.mask_token='<mask>'\n",
            "\tSequence Separator: \n",
            "\t\ttemp_tokenizer.sep_token='</s>'\n",
            "\tClass of Input: \n",
            "\t\ttemp_tokenizer.cls_token='<s>'\n",
            "\n",
            "\n",
            "xlm-roberta-base\n",
            "\tUnknown: \n",
            "\t\ttemp_tokenizer.unk_token='<unk>'\n",
            "\tBeginning of Sequence: \n",
            "\t\ttemp_tokenizer.bos_token='<s>'\n",
            "\tEnd of Sequence: \n",
            "\t\ttemp_tokenizer.eos_token='</s>'\n",
            "\tMask: \n",
            "\t\ttemp_tokenizer.mask_token='<mask>'\n",
            "\tSequence Separator: \n",
            "\t\ttemp_tokenizer.sep_token='</s>'\n",
            "\tClass of Input: \n",
            "\t\ttemp_tokenizer.cls_token='<s>'\n",
            "\n",
            "\n",
            "google/pegasus-xsum\n",
            "\tUnknown: \n",
            "\t\ttemp_tokenizer.unk_token='<unk>'\n",
            "\tBeginning of Sequence: \n",
            "\t\ttemp_tokenizer.bos_token=None\n",
            "\tEnd of Sequence: \n",
            "\t\ttemp_tokenizer.eos_token='</s>'\n",
            "\tMask: \n",
            "\t\ttemp_tokenizer.mask_token='<mask_2>'\n",
            "\tSequence Separator: \n",
            "\t\ttemp_tokenizer.sep_token=None\n",
            "\tClass of Input: \n",
            "\t\ttemp_tokenizer.cls_token=None\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hPhWplGKTget"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
